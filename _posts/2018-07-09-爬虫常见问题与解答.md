---
layout:     post
title:      爬虫常见问题与解答
subtitle:   
date:       2018-07-09
author:     Menhaei
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - python
---
**--题目--****（1） 列举python网络爬虫所引用的模块包，提取数据用到的模块(至少各2个)**

　　requests、urllib | jsonpath xpath beautifulsoup 

**（2）浏览器请求某网站时，从输入到页面显示出来，描述一下请求过程**

　　1.敲域名回车　　2.查询本地的DNS缓存，以找到域名对应的主机IP地址（若有，则跳到4）　　3.查询远程域名根DNS，找到IP地址　　4.向远程IP地址的服务器发送请求（若请求失败且未经过2，则返回2，若再再次失败，返回错误代码）　　5.服务器响应请求，向用户发送数据　　6.浏览器对返回的数据进行处理（浏览器渲染）　　7.显示

**（3） 爬虫在爬取数据，网站有反爬时如何解决**

　　通过headers反爬虫：解决策略，伪造headers　　基于用户行为反爬虫：动态变化去爬取数据，模拟普通用户的行为　　基于动态页面的反爬虫：跟踪服务器发送的ajax请求，模拟ajax请求

**（4） 如果爬数据时ip被封了，如何解决**

　　1、放慢抓取速度，减小对于目标网站造成的压力。但是这样会减少单位时间类的抓取量。　　2、第二种方法是通过设置代理IP等手段，突破反爬虫机制继续高频率抓取。需多个稳定的代理IP

**（5）如何解决登录时有验证码的问题，列出你知道的状态码(至少3个)**

　　 一、筛选得到隐藏信息　　　　进入开发者工具（按F12），找到其中的Network后，手动的先进行一次登录，找到其中的第一个请求，在Header的底部会有一个data的数据段，这个就是登录所需的信息。如果想对其中的隐藏信息进行修改先获取网页Html的内容　　二、将信息进行提交　　找到源码中提交表单所需要的action,和method使用　　三、获取登录后的信息　　信息提交后模拟登录就成功了，接下来就可以获取登录后的信息了，如果有验证码，则提取其中的cookie，请求时携带cookie访问。

　　状态码：　　200 OK：请求成功　　301 重定向所请求的页面已经转移至新的url。　　403 Forbidden  对被请求页面的访问被禁止　　404 Not Found  服务器无法找到被请求的页面　　500 Internal Server Error  请求未完成。服务器遇到不可预知的情况　　504 Gateway Timeout  网关超时

**（6） 什么是进程 线程 协程**

　　进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。　　线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度(标准线程是的)。　　协程和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度。

　　参考网址： https://blog.csdn.net/hairetz/article/details/16119911 

**（7）自动化测试工具用什么包，如何使用有什么功能**

　　selenium+phantomJS 一个是自动化测试工具，一个是无界面的浏览器，他俩配合可以模拟一个用户　　鼠标点击浏览器的动作，从而在其中获取到相应数据

**（8） scrapy中的pipeline 有什么作用 **

　　pipeline(管道)：它负责处理spider中获取item，并进行后期处理（如过滤，存储）

**（9）模拟抓取某网站的第一页，输出匹配公告名字****		　　要求：网址：baseurl，匹配：title**

　　

```
import urllib.request 
import re 

def fetch(baseUrl): 

# 第1步：模拟浏览器发送请求 
data = urllib.request.urlopen(baseUrl).read() #二进制字节形式 
data = data.decode('utf-8') 

# 第2步：页面返回后，利用正则表达式提取想要的内容 
nameList=[] 
nameList = re.compile(r'target="_blank" title="(.*?)"',re.S).search(data) 

# 第3步：返回在页面上析取的标题名 
return nameList 

####### 执行 ######## 
if __name__ =="__main__": 

#要抓取的网页地址 
url = "http://www.sxszbb.com/sxztb/jyxx/001001/MoreInfo.aspx?CategoryNum=001001" 

#存放到名字列表中 
NameList = fetch(url) 

# 输出 NameList 
Length = len(NameList) 
for i in range(0, Length): 
print("标题名%d:%s\n"%(i+1, NameList[i]))
```

		
